# Emotion Detection Testing Suite

This folder contains tools to collect data and analyze why sad/angry emotions aren't being detected properly.

## üéØ Purpose

The main emotion detector works for **happy**, **neutral**, and **surprised**, but struggles with **sad** and **angry**. This testing suite helps us:

1. **Collect real data** about what Google Vision API actually returns
2. **Compare your expressions** vs API detection
3. **Analyze patterns** to understand the problem
4. **Improve detection thresholds** based on real data

## üìÅ Files

### `emotion_data_collector.py` - Main Data Collection Tool
- **Split screen interface**: Camera on left, API data on right
- **Slow detection**: 8 seconds between API calls (plenty of time to type)
- **User input**: Type what expression you're making
- **CSV logging**: Saves everything for analysis

### `data_analyzer.py` - Analysis Tool
- Analyzes collected CSV data
- Shows accuracy rates for each emotion
- Suggests threshold improvements
- Identifies patterns and problems

### `emotion_data_collection.csv` - Data File
- Auto-generated when you run the collector
- Contains all API responses and your verdicts
- Used for analysis and improvements

## üöÄ How to Use

### Step 1: Collect Data
```bash
cd "d:\volume E\gen-ai\face recognition\testing"
"D:/volume E/gen-ai/myenv/Scripts/python.exe" emotion_data_collector.py
```

**What you'll see:**
- Split screen window opens
- Camera feed on left
- API response data on right
- Every 8 seconds, you'll be prompted to type your expression

**What to do:**
1. Make a facial expression (sad, angry, happy, etc.)
2. Wait for detection (every 8 seconds)
3. Type what expression you were making in the console
4. Repeat for 10-20 samples of each emotion
5. Press ESC when done

### Step 2: Analyze Data
```bash
"D:/volume E/gen-ai/myenv/Scripts/python.exe" data_analyzer.py
```

This will show you:
- How often each emotion is correctly detected
- What scores Google Vision API gives for each expression
- Suggested threshold improvements
- Patterns and problems

## üìä What Data is Collected

For each detection, we save:
- **Timestamp**: When the detection happened
- **Your input**: What expression you said you were making
- **API scores**: All 4 emotion scores (Joy, Sorrow, Anger, Surprise)
- **API text**: Human-readable confidence levels
- **System detection**: What our current algorithm detected

## üé≠ Expression Tips

### For **SAD** expressions:
- Drop corners of mouth downward
- Make eyes droopy/half-closed
- Lower and furrow eyebrows
- Look downward or tired
- Hold for 5+ seconds

### For **ANGRY** expressions:
- Furrow eyebrows tightly together
- Narrow or squint eyes
- Tighten lips or slight scowl
- Clench jaw
- Look intense/confrontational
- Hold for 5+ seconds

## üîç Expected Findings

Based on initial testing, we expect to find:
- **Sad expressions**: API gives very low Sorrow scores (0-1)
- **Angry expressions**: API gives very low Anger scores (0-1)
- **Current thresholds**: Too high for sad/angry detection
- **Solution**: Lower thresholds or use alternative detection methods

## üìà After Data Collection

Once you've collected 10-20 samples of sad and angry expressions:

1. **Review the CSV file** to see actual API scores
2. **Run the analyzer** to get improvement suggestions
3. **Update thresholds** in the main emotion detector
4. **Test the improvements** with the enhanced detector

## üéØ Goal

Achieve reliable detection of all 5 emotions:
- ‚úÖ Happy (already working)
- ‚úÖ Neutral (already working) 
- ‚úÖ Surprised (already working)
- üîÑ Sad (needs improvement)
- üîÑ Angry (needs improvement)

## üö® If Google Vision API Can't Detect Sad/Angry

If after data collection we find that Google Vision API consistently gives 0 scores for sad/angry, we may need to:

1. **Use facial landmarks** instead of emotion detection
2. **Combine multiple features** (mouth shape, eyebrow position, etc.)
3. **Use a different AI model** or library
4. **Train our own emotion classifier**

This testing suite will give us the data to make that decision!
